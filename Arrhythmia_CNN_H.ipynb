{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Arrhythmia_CNN_H.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"xwV3biilSwt-"},"source":["## Refernce\n","[Paper](https://www.sciencedirect.com/science/article/abs/pii/S0010482518302713) <br>\n","[Link](https://github.com/tom-beer/Arrhythmia-CNN)"]},{"cell_type":"code","metadata":{"id":"LU-xTGRA-kNo","executionInfo":{"status":"ok","timestamp":1617006860232,"user_tz":-480,"elapsed":932,"user":{"displayName":"old time","photoUrl":"","userId":"08992271775329336379"}}},"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"StofVnpuSlOH","executionInfo":{"status":"ok","timestamp":1617006860733,"user_tz":-480,"elapsed":1424,"user":{"displayName":"old time","photoUrl":"","userId":"08992271775329336379"}}},"source":["# !git clone https://github.com/tom-beer/Arrhythmia-CNN.git\n","# %cd Arrhythmia-CNN/"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"0gR-0fpXTwIO","executionInfo":{"status":"ok","timestamp":1617006860734,"user_tz":-480,"elapsed":1422,"user":{"displayName":"old time","photoUrl":"","userId":"08992271775329336379"}}},"source":["from __future__ import print_function\n","import torch\n","import torch.utils.data\n","import numpy as np\n","import pandas as pd\n","\n","from torch import nn, optim\n","from torch.utils.data.dataset import Dataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"47MoP6UNT0qD","executionInfo":{"status":"ok","timestamp":1617006860734,"user_tz":-480,"elapsed":1418,"user":{"displayName":"old time","photoUrl":"","userId":"08992271775329336379"}}},"source":["is_cuda = True\n","num_epochs = 100\n","batch_size = 10\n","torch.manual_seed(46)\n","log_interval = 10\n","in_channels_ = 1\n","num_segments_in_record = 100\n","segment_len = 3600\n","num_records = 48\n","num_classes = 16\n","allow_label_leakage = True\n","\n","device = torch.device(\"cuda:0\" if is_cuda else \"cpu\")"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L8qoVJM8Sv66","outputId":"7aa37f5d-aa66-4c60-dc7f-2f781b789744"},"source":["# train_ids, test_ids = train_test_split(np.arange(index_set), train_size=.8, random_state=46)\n","# scaler = MinMaxScaler(feature_range=(0, 1), copy=False)\n","\n","class CustomDatasetFromCSV(Dataset):\n","    def __init__(self, data_path, transforms_=None):\n","        self.df = pd.read_pickle(data_path)\n","        self.transforms = transforms_\n","\n","    def __getitem__(self, index):\n","        row = self.df.iloc[index]\n","        signal = row['signal']\n","        target = row['target']\n","        if self.transforms is not None:\n","            signal = self.transforms(signal)\n","        signal = signal.reshape(1, signal.shape[0])\n","        return signal, target\n","\n","    def __len__(self):\n","        return self.df.shape[0]\n","\n","\n","train_dataset = CustomDatasetFromCSV('/content/drive/MyDrive/Arrhythmia-CNN-master/data/Arrhythmia_dataset.pkl')\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False)\n","test_dataset = CustomDatasetFromCSV('/content/drive/MyDrive/Arrhythmia-CNN-master/data/Arrhythmia_dataset.pkl')\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n","\n","\n","class Flatten(torch.nn.Module):\n","    def forward(self, x):\n","        batch_size = x.shape[0]\n","        return x.view(batch_size, -1)\n","\n","\n","def basic_layer(in_channels, out_channels, kernel_size, batch_norm=False, max_pool=True, conv_stride=1, padding=0\n","                , pool_stride=2, pool_size=2):\n","    layer = nn.Sequential(\n","        nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=conv_stride,\n","                  padding=padding),\n","        nn.ReLU())\n","    if batch_norm:\n","        layer = nn.Sequential(\n","            layer,\n","            nn.BatchNorm1d(num_features=out_channels))\n","    if max_pool:\n","        layer = nn.Sequential(\n","            layer,\n","            nn.MaxPool1d(kernel_size=pool_size, stride=pool_stride))\n","\n","    return layer\n","\n","\n","class arrhythmia_classifier(nn.Module):\n","    def __init__(self, in_channels=in_channels_):\n","        super(arrhythmia_classifier, self).__init__()\n","        self.cnn = nn.Sequential(\n","            basic_layer(in_channels=in_channels, out_channels=128, kernel_size=50, batch_norm=True, max_pool=True,\n","                        conv_stride=3, pool_stride=3),\n","            basic_layer(in_channels=128, out_channels=32, kernel_size=7, batch_norm=True, max_pool=True,\n","                        conv_stride=1, pool_stride=2),\n","            basic_layer(in_channels=32, out_channels=32, kernel_size=10, batch_norm=False, max_pool=False,\n","                        conv_stride=1),\n","            basic_layer(in_channels=32, out_channels=128, kernel_size=5, batch_norm=False, max_pool=True,\n","                        conv_stride=2, pool_stride=2),\n","            basic_layer(in_channels=128, out_channels=256, kernel_size=15, batch_norm=False, max_pool=True,\n","                        conv_stride=1, pool_stride=2),\n","            basic_layer(in_channels=256, out_channels=512, kernel_size=5, batch_norm=False, max_pool=False,\n","                        conv_stride=1),\n","            basic_layer(in_channels=512, out_channels=128, kernel_size=3, batch_norm=False, max_pool=False,\n","                        conv_stride=1),\n","            Flatten(),\n","            nn.Linear(in_features=1152, out_features=512),\n","            nn.ReLU(),\n","            nn.Dropout(p=.1),\n","            nn.Linear(in_features=512, out_features=num_classes),\n","            nn.Softmax()\n","        )\n","\n","    def forward(self, x, ex_features=None):\n","        return self.cnn(x)\n","\n","\n","def calc_next_len_conv1d(current_len=112500, kernel_size=16, stride=8, padding=0, dilation=1):\n","    return int(np.floor((current_len + 2 * padding - dilation * (kernel_size - 1) - 1) / stride + 1))\n","\n","\n","model = arrhythmia_classifier().to(device).double()\n","lr = 3e-1\n","num_of_iteration = len(train_dataset) // batch_size\n","\n","optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-6)\n","criterion = nn.NLLLoss()\n","\n","\n","def train(epoch):\n","    model.train()\n","    train_loss = 0\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = criterion(output, target)\n","        loss.backward()\n","        train_loss += loss.item()\n","        optimizer.step()\n","        if batch_idx % log_interval == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                       100. * batch_idx / len(train_loader),\n","                       loss.item() / len(data)))\n","\n","    print('====> Epoch: {} Average loss: {:.4f}'.format(\n","        epoch, train_loss / len(train_loader.dataset)))\n","\n","\n","def test(epoch):\n","    model.eval()\n","    test_loss = 0\n","    with torch.no_grad():\n","        all_=0\n","        t_=0\n","        for batch_idx, (data, target) in enumerate(test_loader):\n","            all_+=1\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            loss = criterion(output, target)\n","            test_loss += loss.item()\n","\n","            if batch_idx == 0:\n","                n = min(data.size(0), 4)\n","\n","            list_=output[0].tolist()\n","            pridict_=int(list_.index(max(list_)))\n","            if int(target)==pridict_:\n","                t_+=1\n","    accuracy_=t_/all_\n","    test_loss /= len(test_loader.dataset)\n","    print('====> Test set loss: {:.5f}'.format(test_loss))\n","    print('====> Test Accuracy: {:.5f}'.format(accuracy_))\n","    # print(f'Learning rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n","for epoch in range(1, num_epochs + 1):\n","  train(epoch)\n","  test(epoch)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:119: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Train Epoch: 1 [0/823 (0%)]\tLoss: -0.006458\n","Train Epoch: 1 [100/823 (12%)]\tLoss: -0.100000\n","Train Epoch: 1 [200/823 (24%)]\tLoss: -0.100000\n","Train Epoch: 1 [300/823 (36%)]\tLoss: 0.000000\n","Train Epoch: 1 [400/823 (48%)]\tLoss: 0.000000\n","Train Epoch: 1 [500/823 (60%)]\tLoss: 0.000000\n","Train Epoch: 1 [600/823 (72%)]\tLoss: 0.000000\n","Train Epoch: 1 [700/823 (84%)]\tLoss: 0.000000\n","Train Epoch: 1 [800/823 (96%)]\tLoss: 0.000000\n","====> Epoch: 1 Average loss: -0.0332\n","====> Test set loss: -0.34386\n","====> Test Accuracy: 0.34386\n","Train Epoch: 2 [0/823 (0%)]\tLoss: -0.100000\n","Train Epoch: 2 [100/823 (12%)]\tLoss: -0.100000\n","Train Epoch: 2 [200/823 (24%)]\tLoss: -0.100000\n","Train Epoch: 2 [300/823 (36%)]\tLoss: -0.000000\n","Train Epoch: 2 [400/823 (48%)]\tLoss: -0.000000\n","Train Epoch: 2 [500/823 (60%)]\tLoss: -0.000000\n","Train Epoch: 2 [600/823 (72%)]\tLoss: -0.000000\n","Train Epoch: 2 [700/823 (84%)]\tLoss: -0.000000\n","Train Epoch: 2 [800/823 (96%)]\tLoss: -0.000000\n","====> Epoch: 2 Average loss: -0.0344\n","====> Test set loss: -0.34386\n","====> Test Accuracy: 0.34386\n","Train Epoch: 3 [0/823 (0%)]\tLoss: -0.100000\n","Train Epoch: 3 [100/823 (12%)]\tLoss: -0.100000\n","Train Epoch: 3 [200/823 (24%)]\tLoss: -0.100000\n","Train Epoch: 3 [300/823 (36%)]\tLoss: -0.000000\n","Train Epoch: 3 [400/823 (48%)]\tLoss: -0.000000\n","Train Epoch: 3 [500/823 (60%)]\tLoss: -0.000000\n","Train Epoch: 3 [600/823 (72%)]\tLoss: -0.000000\n","Train Epoch: 3 [700/823 (84%)]\tLoss: -0.000000\n","Train Epoch: 3 [800/823 (96%)]\tLoss: -0.000000\n","====> Epoch: 3 Average loss: -0.0344\n"],"name":"stdout"}]}]}